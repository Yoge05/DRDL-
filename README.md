
Deep Learning & Reinforcement Learning Assignment

| Field            | Details                                  |
| ---------------- | ---------------------------------------- |
|   Name           | Yogeshwaran P                            |
|   USN            | 1CD22AI063                               |
|   Subject        | Deep Learning and Reinforcement Learning |
|  Subject Code    | BAI701                                   |

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
1ï¸âƒ£ AlexNet.py â€” CNN Image Classification
ğŸ”¹ Original Code

Standard AlexNet-style CNN

ReLU activation

Large kernels (11Ã—11)

Flatten + Dense layers

No explicit architectural optimization

ğŸ”¹ Modified Code (Friend Version â€“ UNIQUE)

Exact Changes Made

Replaced ReLU with LeakyReLU

Added Batch Normalization after every convolution layer

Changed first convolution kernel size 11Ã—11 â†’ 7Ã—7

Replaced Flatten with GlobalAveragePooling

Reduced dense layer dependency

ğŸ”¹ Why These Changes Are Effective

Prevents vanishing gradient issues

Faster and more stable convergence

Fewer parameters â†’ lower memory usage

Model summary becomes structurally different

âœ… Uniqueness: Architecture-level change (not just tuning values)

2ï¸âƒ£ TicTacToe.py â€” Reinforcement Learning Game
ğŸ”¹ Original Code

Incorrect player symbol initialization

Weak reward structure

High randomness even after training

Poor input validation

Game sometimes continued after win/draw

ğŸ”¹ Modified Code

Exact Changes Made

Fixed player symbol: 20 â†’ 1

Improved reward system:

Win = +1

Loss = âˆ’1

Draw = +0.3

Reduced exploration rate: 0.3 â†’ 0.1

Increased learning rate: 0.2 â†’ 0.3

Reduced training rounds: 50000 â†’ 20000

Enhanced board display (X O _ format)

Added input validation

Fixed game termination logic

ğŸ”¹ Why These Changes Are Effective

Faster convergence

Smarter AI decisions

Stable gameplay

Better user interaction

3ï¸âƒ£ RNN.py â€” Character-Level Text Generation
ğŸ”¹ Original Code

Short training text

ReLU activation

Argmax-based text generation

No regularization

Limited creativity

ğŸ”¹ Modified Code

Exact Changes Made

Changed training text to a technical sentence

Increased sequence length: 5 â†’ 6

Changed activation: ReLU â†’ tanh

Increased hidden units: 64

Added Dropout (0.3)

Custom Adam learning rate

Introduced temperature-based probabilistic sampling

Increased generated text length

ğŸ”¹ Why These Changes Are Effective

Stable RNN training

Less repetitive text

More natural and creative output

Output becomes non-deterministic

âœ… Uniqueness: Temperature-based sampling (advanced concept)

4ï¸âƒ£ LSTM.py â€” Time Series Forecasting (Airline Passengers)
ğŸ”¹ Original Code

Local dataset dependency

Single LSTM layer

Incorrect input shape

Batch size = 1

No validation or early stopping

ğŸ”¹ Modified Code

Exact Changes Made

Dataset loaded from GitHub (portable)

Increased sequence length: 10 â†’ 12

Corrected input shape (TIME_STEPS, 1)

Introduced stacked LSTM layers

Added Dropout

Added EarlyStopping

Improved batch size: 1 â†’ 16

Added MAE along with RMSE

Improved prediction visualization

ğŸ”¹ Why These Changes Are Effective

Better temporal modeling

Reduced overfitting

Faster and stable training

Improved forecasting accuracy

5ï¸âƒ£ DeepReinforcementLearning.py â€” Q-Learning on Graph
ğŸ”¹ Original Code

Sparse rewards (only goal reward)

Unstable Q-update rule

Pure random exploration

Complex environment heuristics (police/drugs)

Noisy learning curve

ğŸ”¹ Modified Code

Exact Changes Made

Added step penalty (âˆ’1)

Used standard Q-learning update rule

Introduced learning rate (Î±)

Increased discount factor (Î³)

Implemented epsilon-greedy exploration with decay

Tracked average Q-value instead of sum

Removed environment-specific heuristics

ğŸ”¹ Why These Changes Are Effective

Encourages shortest path

Stable convergence

Smooth learning curve

Cleaner, algorithm-focused logic
